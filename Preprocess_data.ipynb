{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9231ba2-e98d-4e96-b921-54e05783fd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b50f45b-b4a6-4c9b-a5c6-ce0d10d023d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType, StringType\n",
    "from pyspark.sql.functions import col, dayofweek, month, hour, from_unixtime, udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AmazonReviews\")\n",
    "         .config(\"spark.driver.memory\", \"6g\")  \n",
    "         .config(\"spark.executor.memory\", \"6g\")  \n",
    "         .config(\"spark.executor.instances\", \"2\") \n",
    "         .getOrCreate())\n",
    "\n",
    "# check spark session is correctly initialized\n",
    "if spark is None:\n",
    "    raise RuntimeError(\"SparkSession not initialized\")\n",
    "\n",
    "review_filepath = './data/Gift_Cards.jsonl'\n",
    "metadata_filepath = './data/meta_Gift_Cards.jsonl'\n",
    "\n",
    "reviews_df = spark.read.json(review_filepath)\n",
    "metadata_df = spark.read.json(metadata_filepath)\n",
    "\n",
    "# filter reviews with rating > 3\n",
    "reviews_df = reviews_df.filter(col(\"rating\") > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82843fa2-a1df-4b70-bab3-3577b3ae02ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053\n",
      "1137\n"
     ]
    }
   ],
   "source": [
    "print(reviews_df.select(\"parent_asin\").distinct().count())\n",
    "print(metadata_df.select(\"parent_asin\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71fc7993-57f8-4f1c-8ef5-a97a1f18173f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful_vote: long (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- attachment_type: string (nullable = true)\n",
      " |    |    |-- large_image_url: string (nullable = true)\n",
      " |    |    |-- medium_image_url: string (nullable = true)\n",
      " |    |    |-- small_image_url: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reviews_df.printSchema()\n",
    "# metadata_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5e7937f-3ed1-45c6-9a3b-66cfc19482c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful_vote: long (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- attachment_type: string (nullable = true)\n",
      " |    |    |-- large_image_url: string (nullable = true)\n",
      " |    |    |-- medium_image_url: string (nullable = true)\n",
      " |    |    |-- small_image_url: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- rating: double (nullable = false)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      "\n",
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- average_rating: double (nullable = false)\n",
      " |-- bought_together: string (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- details: struct (nullable = true)\n",
      " |    |-- Age Range (Description): string (nullable = true)\n",
      " |    |-- Assembly required: string (nullable = true)\n",
      " |    |-- Batteries required: string (nullable = true)\n",
      " |    |-- Best Sellers Rank: struct (nullable = true)\n",
      " |    |    |-- For Her: long (nullable = true)\n",
      " |    |    |-- Gift Card Holders: long (nullable = true)\n",
      " |    |    |-- Gift Cards: long (nullable = true)\n",
      " |    |    |-- Kids: long (nullable = true)\n",
      " |    |    |-- Thank You & Appreciation: long (nullable = true)\n",
      " |    |-- Brand: string (nullable = true)\n",
      " |    |-- Brand Name: string (nullable = true)\n",
      " |    |-- Capacity: string (nullable = true)\n",
      " |    |-- Color: string (nullable = true)\n",
      " |    |-- Compatible Devices: string (nullable = true)\n",
      " |    |-- Cuisine: string (nullable = true)\n",
      " |    |-- Date First Available: string (nullable = true)\n",
      " |    |-- Department: string (nullable = true)\n",
      " |    |-- Domestic Shipping: string (nullable = true)\n",
      " |    |-- Fabric Type: string (nullable = true)\n",
      " |    |-- Flavor: string (nullable = true)\n",
      " |    |-- Form Factor: string (nullable = true)\n",
      " |    |-- Grade Rating: string (nullable = true)\n",
      " |    |-- Import: string (nullable = true)\n",
      " |    |-- Included Components: string (nullable = true)\n",
      " |    |-- International Shipping: string (nullable = true)\n",
      " |    |-- Is Autographed: string (nullable = true)\n",
      " |    |-- Is Discontinued By Manufacturer: string (nullable = true)\n",
      " |    |-- Item Form: string (nullable = true)\n",
      " |    |-- Item Weight: string (nullable = true)\n",
      " |    |-- Item model number: string (nullable = true)\n",
      " |    |-- Manufacturer: string (nullable = true)\n",
      " |    |-- Manufacturer Part Number: string (nullable = true)\n",
      " |    |-- Manufacturer recommended age: string (nullable = true)\n",
      " |    |-- Material: string (nullable = true)\n",
      " |    |-- Material Type: string (nullable = true)\n",
      " |    |-- Material Type Free: string (nullable = true)\n",
      " |    |-- Memory Storage Capacity: string (nullable = true)\n",
      " |    |-- Number of Items: string (nullable = true)\n",
      " |    |-- Number of Pieces: string (nullable = true)\n",
      " |    |-- Occasion: string (nullable = true)\n",
      " |    |-- Our Recommended age: string (nullable = true)\n",
      " |    |-- Package Dimensions: string (nullable = true)\n",
      " |    |-- Package Information: string (nullable = true)\n",
      " |    |-- Package Weight: string (nullable = true)\n",
      " |    |-- Paper Finish: string (nullable = true)\n",
      " |    |-- Paper Weight: string (nullable = true)\n",
      " |    |-- Part Number: string (nullable = true)\n",
      " |    |-- Pattern: string (nullable = true)\n",
      " |    |-- Power Source: string (nullable = true)\n",
      " |    |-- Pre-printed: string (nullable = true)\n",
      " |    |-- Pricing: string (nullable = true)\n",
      " |    |-- Product Care Instructions: string (nullable = true)\n",
      " |    |-- Product Dimensions: string (nullable = true)\n",
      " |    |-- Seasons: string (nullable = true)\n",
      " |    |-- Shape: string (nullable = true)\n",
      " |    |-- Sheet Size: string (nullable = true)\n",
      " |    |-- Size: string (nullable = true)\n",
      " |    |-- Special Feature: string (nullable = true)\n",
      " |    |-- Specialty: string (nullable = true)\n",
      " |    |-- Style: string (nullable = true)\n",
      " |    |-- Surface Recommendation: string (nullable = true)\n",
      " |    |-- Tank Volume: string (nullable = true)\n",
      " |    |-- Target Audience: string (nullable = true)\n",
      " |    |-- Theme: string (nullable = true)\n",
      " |    |-- Total Eaches: string (nullable = true)\n",
      " |    |-- Type of item: string (nullable = true)\n",
      " |    |-- UPC: string (nullable = true)\n",
      " |    |-- Unit Count: string (nullable = true)\n",
      " |    |-- Warranty Description: string (nullable = true)\n",
      " |    |-- Water Resistance Level: string (nullable = true)\n",
      " |    |-- Wattage: string (nullable = true)\n",
      " |    |-- Whiteness: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- hi_res: string (nullable = true)\n",
      " |    |    |-- large: string (nullable = true)\n",
      " |    |    |-- thumb: string (nullable = true)\n",
      " |    |    |-- variant: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- price: double (nullable = false)\n",
      " |-- rating_number: long (nullable = true)\n",
      " |-- store: string (nullable = true)\n",
      " |-- subtitle: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- videos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill NaN values with 0\n",
    "reviews_df = reviews_df.fillna(0)\n",
    "metadata_df = metadata_df.fillna(0)\n",
    "\n",
    "# reviews_df.printSchema()\n",
    "# metadata_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b13910dc-6c81-4f81-a07b-a0223aec423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/24 12:33:59 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[author: string, average_rating: double, bought_together: string, categories: array<string>, description: array<string>, details: struct<Age Range (Description):string,Assembly required:string,Batteries required:string,Best Sellers Rank:struct<For Her:bigint,Gift Card Holders:bigint,Gift Cards:bigint,Kids:bigint,Thank You & Appreciation:bigint>,Brand:string,Brand Name:string,Capacity:string,Color:string,Compatible Devices:string,Cuisine:string,Date First Available:string,Department:string,Domestic Shipping:string,Fabric Type:string,Flavor:string,Form Factor:string,Grade Rating:string,Import:string,Included Components:string,International Shipping:string,Is Autographed:string,Is Discontinued By Manufacturer:string,Item Form:string,Item Weight:string,Item model number:string,Manufacturer:string,Manufacturer Part Number:string,Manufacturer recommended age:string,Material:string,Material Type:string,Material Type Free:string,Memory Storage Capacity:string,Number of Items:string,Number of Pieces:string,Occasion:string,Our Recommended age:string,Package Dimensions:string,Package Information:string,Package Weight:string,Paper Finish:string,Paper Weight:string,Part Number:string,Pattern:string,Power Source:string,Pre-printed:string,Pricing:string,Product Care Instructions:string,Product Dimensions:string,Seasons:string,Shape:string,Sheet Size:string,Size:string,Special Feature:string,Specialty:string,Style:string,Surface Recommendation:string,Tank Volume:string,Target Audience:string,Theme:string,Total Eaches:string,Type of item:string,UPC:string,Unit Count:string,Warranty Description:string,Water Resistance Level:string,Wattage:string,Whiteness:string>, features: array<string>, images: array<struct<hi_res:string,large:string,thumb:string,variant:string>>, main_category: string, parent_asin: string, price: double, rating_number: bigint, store: string, subtitle: string, title: string, videos: array<struct<title:string,url:string,user_id:string>>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = reviews_df.repartition(10)\n",
    "metadata_df = metadata_df.repartition(10)\n",
    "\n",
    "# cache\n",
    "reviews_df.cache()\n",
    "metadata_df.cache()\n",
    "\n",
    "# # checkpoint to truncate the lineage of df and prevent long lineage issues\n",
    "# spark.sparkContext.setCheckpointDir(\"/path/to/checkpoint/dir\")\n",
    "# reviews_df = reviews_df.checkpoint()\n",
    "# metadata_df = metadata_df.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1208fdba-ad36-4365-9f10-bca76cb7c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, ArrayType\n",
    "from pyspark.sql.functions import col, dayofweek, month, from_unixtime, udf\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Define schema for reviews\n",
    "reviews_schema = StructType([\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"overall\", DoubleType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"summary\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", LongType(), True),\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for metadata\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"features\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"details\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Initialize Spark session with increased memory settings\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AmazonReviews\")\n",
    "         .config(\"spark.driver.memory\", \"8g\")  \n",
    "         .config(\"spark.executor.memory\", \"8g\")  \n",
    "         .config(\"spark.executor.instances\", \"2\")\n",
    "         .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "         .config(\"spark.memory.offHeap.enabled\", True)\n",
    "         .config(\"spark.memory.offHeap.size\", \"4g\")\n",
    "         .config(\"spark.storage.memoryFraction\", \"0.8\")\n",
    "         .config(\"spark.shuffle.memoryFraction\", \"0.4\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Check if Spark session is correctly initialized\n",
    "if spark is None:\n",
    "    raise RuntimeError(\"SparkSession not initialized\")\n",
    "\n",
    "# Read JSON files with the defined schema\n",
    "reviews_df = spark.read.schema(reviews_schema).json(review_filepath)\n",
    "metadata_df = spark.read.schema(metadata_schema).json(metadata_filepath)\n",
    "\n",
    "# Filter reviews with rating > 3\n",
    "reviews_df = reviews_df.filter(col(\"rating\") > 3)\n",
    "\n",
    "print(reviews_df.select(\"asin\").distinct().count())\n",
    "print(metadata_df.select(\"asin\").distinct().count())\n",
    "\n",
    "# Fill NaN values with 0\n",
    "reviews_df = reviews_df.fillna(0)\n",
    "metadata_df = metadata_df.fillna(0)\n",
    "\n",
    "reviews_df.printSchema()\n",
    "metadata_df.printSchema()\n",
    "\n",
    "reviews_df = reviews_df.repartition(10)\n",
    "metadata_df = metadata_df.repartition(10)\n",
    "\n",
    "# Cache DataFrames\n",
    "reviews_df.cache()\n",
    "metadata_df.cache()\n",
    "\n",
    "# Convert timestamp from ms to s and then to timestamp type\n",
    "reviews_df = reviews_df.withColumn(\"timestamp\", from_unixtime(col(\"unixReviewTime\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# Extract temporal features\n",
    "reviews_df = reviews_df.withColumn(\"day_of_week\", dayofweek(col(\"timestamp\")))\n",
    "reviews_df = reviews_df.withColumn(\"month\", month(col(\"timestamp\")))\n",
    "\n",
    "# Drop video and image columns\n",
    "metadata_df = metadata_df.drop('video', 'image')\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen1.5-0.5B')\n",
    "model = AutoModel.from_pretrained('Qwen/Qwen1.5-0.5B')\n",
    "\n",
    "def generate_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).numpy().tolist()\n",
    "    return embeddings\n",
    "\n",
    "generate_embeddings_udf = udf(lambda x: generate_embeddings(x), ArrayType(DoubleType()))\n",
    "\n",
    "# Generate embeddings for textual features using mapPartitions\n",
    "def process_partition(partition):\n",
    "    for row in partition:\n",
    "        row[\"text_embedding\"] = generate_embeddings(row[\"text\"])\n",
    "        row[\"title_embedding\"] = generate_embeddings(row[\"title\"])\n",
    "        yield row\n",
    "\n",
    "reviews_rdd = reviews_df.rdd.mapPartitions(process_partition)\n",
    "reviews_df = spark.createDataFrame(reviews_rdd, schema=reviews_df.schema)\n",
    "\n",
    "# Apply similar processing for other columns as needed\n",
    "def process_metadata_partition(partition):\n",
    "    for row in partition:\n",
    "        row[\"features_embedding\"] = generate_embeddings(row[\"features\"])\n",
    "        row[\"description_embedding\"] = generate_embeddings(row[\"description\"])\n",
    "        row[\"details_embedding\"] = generate_embeddings(row[\"details\"])\n",
    "        yield row\n",
    "\n",
    "metadata_rdd = metadata_df.rdd.mapPartitions(process_metadata_partition)\n",
    "metadata_df = spark.createDataFrame(metadata_rdd, schema=metadata_df.schema)\n",
    "\n",
    "# Show resulting DataFrames\n",
    "reviews_df.show()\n",
    "metadata_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610fae8f-efaa-40b1-9da0-dc6ddeda383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- overall: double (nullable = false)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- price: double (nullable = false)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- features: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- details: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/24 13:14:53 WARN CacheManager: Asked to cache already cached data.\n",
      "24/07/24 13:14:53 WARN CacheManager: Asked to cache already cached data.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, ArrayType\n",
    "from pyspark.sql.functions import col, dayofweek, month, hour, from_unixtime, udf\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Define schema for reviews\n",
    "reviews_schema = StructType([\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"overall\", DoubleType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"summary\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", LongType(), True),\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for metadata\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"features\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"details\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Initialize Spark session with increased memory settings\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AmazonReviews\")\n",
    "         .config(\"spark.driver.memory\", \"6g\")\n",
    "         .config(\"spark.executor.memory\", \"6g\")\n",
    "         .config(\"spark.executor.instances\", \"2\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Check if Spark session is correctly initialized\n",
    "if spark is None:\n",
    "    raise RuntimeError(\"SparkSession not initialized\")\n",
    "\n",
    "# Read JSON files with the defined schema\n",
    "reviews_df = spark.read.schema(reviews_schema).json(review_filepath)\n",
    "metadata_df = spark.read.schema(metadata_schema).json(metadata_filepath)\n",
    "\n",
    "# Filter reviews with rating > 3\n",
    "reviews_df = reviews_df.filter(col(\"rating\") > 3)\n",
    "\n",
    "# print(reviews_df.select(\"asin\").distinct().count())\n",
    "# print(metadata_df.select(\"asin\").distinct().count())\n",
    "\n",
    "# Fill NaN values with 0\n",
    "reviews_df = reviews_df.fillna(0)\n",
    "metadata_df = metadata_df.fillna(0)\n",
    "\n",
    "reviews_df.printSchema()\n",
    "metadata_df.printSchema()\n",
    "\n",
    "reviews_df = reviews_df.repartition(10)\n",
    "metadata_df = metadata_df.repartition(10)\n",
    "\n",
    "# Cache DataFrames\n",
    "reviews_df.cache()\n",
    "metadata_df.cache()\n",
    "\n",
    "# Convert timestamp from ms to s and then to timestamp type\n",
    "reviews_df = reviews_df.withColumn(\"timestamp\", from_unixtime(col(\"unixReviewTime\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# Extract temporal features\n",
    "reviews_df = reviews_df.withColumn(\"day_of_week\", dayofweek(col(\"timestamp\")))\n",
    "reviews_df = reviews_df.withColumn(\"month\", month(col(\"timestamp\")))\n",
    "\n",
    "# Drop video and image columns\n",
    "metadata_df = metadata_df.drop('video', 'image')\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen1.5-0.5B')\n",
    "model = AutoModel.from_pretrained('Qwen/Qwen1.5-0.5B')\n",
    "\n",
    "def generate_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).numpy().tolist()\n",
    "    return embeddings\n",
    "\n",
    "generate_embeddings_udf = udf(lambda x: generate_embeddings(x), ArrayType(DoubleType()))\n",
    "\n",
    "# Generate embeddings for textual features using mapPartitions\n",
    "def process_partition(partition):\n",
    "    for row in partition:\n",
    "        row[\"text_embedding\"] = generate_embeddings(row[\"text\"])\n",
    "        row[\"title_embedding\"] = generate_embeddings(row[\"title\"])\n",
    "        yield row\n",
    "\n",
    "reviews_rdd = reviews_df.rdd.mapPartitions(process_partition)\n",
    "reviews_df = spark.createDataFrame(reviews_rdd, schema=reviews_df.schema)\n",
    "\n",
    "# Apply similar processing for other columns as needed\n",
    "def process_metadata_partition(partition):\n",
    "    for row in partition:\n",
    "        row[\"features_embedding\"] = generate_embeddings(row[\"features\"])\n",
    "        row[\"description_embedding\"] = generate_embeddings(row[\"description\"])\n",
    "        row[\"details_embedding\"] = generate_embeddings(row[\"details\"])\n",
    "        yield row\n",
    "\n",
    "metadata_rdd = metadata_df.rdd.mapPartitions(process_metadata_partition)\n",
    "metadata_df = spark.createDataFrame(metadata_rdd, schema=metadata_df.schema)\n",
    "\n",
    "# Show resulting DataFrames\n",
    "reviews_df.show()\n",
    "metadata_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdaaf6-6ba5-4f89-8853-55b64094ddaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f3e21-5ad7-40f5-838d-51dcb71f4614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d4624f-eb31-4cac-b845-d03dec75030e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b950d87-0007-4fcc-8366-2ce4f859711d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630633b-d1b6-4589-8e8f-3ce3e3c0e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode user_id and asin\n",
    "indexer_user = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_encoded\")\n",
    "indexer_asin = StringIndexer(inputCol=\"asin\", outputCol=\"asin_encoded\")\n",
    "\n",
    "reviews_df = indexer_user.fit(reviews_df).transform(reviews_df)\n",
    "reviews_df = indexer_asin.fit(reviews_df).transform(reviews_df)\n",
    "\n",
    "# merging metadata with reviews based on parent_asin\n",
    "df = reviews_df.join(metadata_df, on='parent_asin', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08b69d5b-b1ba-4cf7-a527-5ffd77090ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "def sentiment_analysis(text):\n",
    "    # Placeholder function for sentiment analysis\n",
    "    # Implement actual sentiment analysis using a model or library\n",
    "    return {'label': 'POSITIVE', 'score': 0.9}  # Example output\n",
    "\n",
    "sentiment_analysis_udf = udf(lambda text: sentiment_analysis(text)['score'] if sentiment_analysis(text)['label'] == 'POSITIVE' else -sentiment_analysis(text)['score'], FloatType())\n",
    "\n",
    "df = df.withColumn(\"sentiment_score\", sentiment_analysis_udf(col(\"text\")))\n",
    "\n",
    "# compute similarity scores\n",
    "def compute_similarity(embeddings):\n",
    "    similarity_matrix = cosine_similarity(np.vstack(embeddings))\n",
    "    return similarity_matrix.mean(axis=1).tolist()\n",
    "\n",
    "compute_similarity_udf = udf(compute_similarity, ArrayType(FloatType()))\n",
    "\n",
    "df = df.withColumn(\"similarity_score\", compute_similarity_udf(col(\"text_embedding\")))\n",
    "\n",
    "# define UDF to convert array to vector\n",
    "def array_to_vector(array):\n",
    "    return Vectors.dense(array)\n",
    "\n",
    "array_to_vector_udf = udf(array_to_vector, VectorUDT())\n",
    "\n",
    "# convert array columns to vector columns\n",
    "df = df.withColumn(\"text_embedding\", array_to_vector_udf(col(\"text_embedding\")))\n",
    "df = df.withColumn(\"title_embedding\", array_to_vector_udf(col(\"title_embedding\")))\n",
    "df = df.withColumn(\"features_embedding\", array_to_vector_udf(col(\"features_embedding\")))\n",
    "df = df.withColumn(\"description_embedding\", array_to_vector_udf(col(\"description_embedding\")))\n",
    "df = df.withColumn(\"details_embedding\", array_to_vector_udf(col(\"details_embedding\")))\n",
    "df = df.withColumn(\"similarity_score\", array_to_vector_udf(col(\"similarity_score\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b560b805-e06e-41a7-91d6-4e6b68f08556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining features into a final feature set\n",
    "features = ['user_id_encoded', 'asin_encoded', 'rating', 'verified_purchase', \n",
    "            'helpful_vote', 'day_of_week', 'month', 'sentiment_score', 'similarity_score']\n",
    "embeddings = ['text_embedding', 'title_embedding', 'features_embedding', 'description_embedding', 'details_embedding']\n",
    "\n",
    "# split into features and targets\n",
    "feature_columns = features + embeddings\n",
    "target_column = 'rating'\n",
    "\n",
    "# create features, target df\n",
    "features_df = df.select(*feature_columns)\n",
    "target_df = df.select(target_column)\n",
    "\n",
    "# features_df.show()\n",
    "# target_df.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7aeb84a-60c1-4e13-b5a2-9fea9ec0a285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/24 10:37:12 WARN CacheManager: Asked to cache already cached data.        \n",
      "24/07/24 10:37:12 WARN CacheManager: Asked to cache already cached data.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "24/07/24 10:37:21 ERROR Executor: Exception in task 3.0 in stage 65.0 (TID 186) \n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/1l/fxxxl8p13clcn13ph4b5wcjc0000gn/T/ipykernel_48152/1915397370.py\", line 42, in <lambda>\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/sql/udf.py\", line 423, in wrapper\n",
      "    return self(*args)\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 339, in __call__\n",
      "    sc = get_active_spark_context()\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "24/07/24 10:37:21 ERROR Executor: Exception in task 7.0 in stage 65.0 (TID 190)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/1l/fxxxl8p13clcn13ph4b5wcjc0000gn/T/ipykernel_48152/1915397370.py\", line 42, in <lambda>\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/sql/udf.py\", line 423, in wrapper\n",
      "    return self(*args)\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 339, in __call__\n",
      "    sc = get_active_spark_context()\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "24/07/24 10:37:21 ERROR Executor: Exception in task 1.0 in stage 65.0 (TID 184)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/1l/fxxxl8p13clcn13ph4b5wcjc0000gn/T/ipykernel_48152/1915397370.py\", line 42, in <lambda>\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/sql/udf.py\", line 423, in wrapper\n",
      "    return self(*args)\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 339, in __call__\n",
      "    sc = get_active_spark_context()\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "24/07/24 10:37:21 ERROR Executor: Exception in task 4.0 in stage 65.0 (TID 187)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/1l/fxxxl8p13clcn13ph4b5wcjc0000gn/T/ipykernel_48152/1915397370.py\", line 42, in <lambda>\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/sql/udf.py\", line 423, in wrapper\n",
      "    return self(*args)\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 339, in __call__\n",
      "    sc = get_active_spark_context()\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "24/07/24 10:37:21 ERROR Executor: Exception in task 0.0 in stage 65.0 (TID 183)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/1l/fxxxl8p13clcn13ph4b5wcjc0000gn/T/ipykernel_48152/1915397370.py\", line 42, in <lambda>\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/sql/udf.py\", line 423, in wrapper\n",
      "    return self(*args)\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 339, in __call__\n",
      "    sc = get_active_spark_context()\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "24/07/24 10:37:21 ERROR Executor: Exception in task 2.0 in stage 65.0 (TID 185)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/1l/fxxxl8p13clcn13ph4b5wcjc0000gn/T/ipykernel_48152/1915397370.py\", line 42, in <lambda>\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/sql/udf.py\", line 423, in wrapper\n",
      "    return self(*args)\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 339, in __call__\n",
      "    sc = get_active_spark_context()\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "24/07/24 10:37:21 ERROR Executor: Exception in task 6.0 in stage 65.0 (TID 189)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/1l/fxxxl8p13clcn13ph4b5wcjc0000gn/T/ipykernel_48152/1915397370.py\", line 42, in <lambda>\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/sql/udf.py\", line 423, in wrapper\n",
      "    return self(*args)\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 339, in __call__\n",
      "    sc = get_active_spark_context()\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "24/07/24 10:37:21 ERROR Executor: Exception in task 5.0 in stage 65.0 (TID 188)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/1l/fxxxl8p13clcn13ph4b5wcjc0000gn/T/ipykernel_48152/1915397370.py\", line 42, in <lambda>\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/sql/udf.py\", line 423, in wrapper\n",
      "    return self(*args)\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 339, in __call__\n",
      "    sc = get_active_spark_context()\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "24/07/24 10:37:21 WARN TaskSetManager: Lost task 1.0 in stage 65.0 (TID 184) (192.168.1.165 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/1l/fxxxl8p13clcn13ph4b5wcjc0000gn/T/ipykernel_48152/1915397370.py\", line 42, in <lambda>\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/sql/udf.py\", line 423, in wrapper\n",
      "    return self(*args)\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 339, in __call__\n",
      "    sc = get_active_spark_context()\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "\n",
      "24/07/24 10:37:21 ERROR TaskSetManager: Task 1 in stage 65.0 failed 1 times; aborting job\n",
      "24/07/24 10:37:21 WARN TaskSetManager: Lost task 8.0 in stage 65.0 (TID 191) (192.168.1.165 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 65.0 failed 1 times, most recent failure: Lost task 1.0 in stage 65.0 (TID 184) (192.168.1.165 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/1l/fxxxl8p13clcn13ph4b5wcjc0000gn/T/ipykernel_48152/1915397370.py\", line 42, in <lambda>\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/sql/udf.py\", line 423, in wrapper\n",
      "    return self(*args)\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 339, in __call__\n",
      "    sc = get_active_spark_context()\n",
      "  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/1l/fxxxl8p13clcn13ph4b5wcjc0000gn/T/ipykernel_48152/1915397370.py\", line 42, in <lambda>\n  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/sql/udf.py\", line 423, in wrapper\n    return self(*args)\n  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 339, in __call__\n    sc = get_active_spark_context()\n  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# assemble all feature columns into a single vector column\u001b[39;00m\n\u001b[1;32m    111\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39mfeature_columns, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43massembler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# train a Random Forest model\u001b[39;00m\n\u001b[1;32m    115\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m, featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, numTrees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/ml/wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/anaconda3/envs/ecrawl/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/1l/fxxxl8p13clcn13ph4b5wcjc0000gn/T/ipykernel_48152/1915397370.py\", line 42, in <lambda>\n  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/sql/udf.py\", line 423, in wrapper\n    return self(*args)\n  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 339, in __call__\n    sc = get_active_spark_context()\n  File \"/Users/vynguyen/anaconda3/envs/ecrawl/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n"
     ]
    }
   ],
   "source": [
    "data = features_df.withColumn(\"rating\", target_df[target_column])\n",
    "\n",
    "# assemble all feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# train a Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"rating\", featuresCol=\"features\", numTrees=100)\n",
    "rf_model = rf.fit(data)\n",
    "\n",
    "# get feature importances\n",
    "importances = rf_model.featureImportances\n",
    "\n",
    "# convert importances to a list of tuples (feature, importance)\n",
    "feature_importances = [(feature, importance) for feature, importance in zip(feature_columns, importances.toArray())]\n",
    "\n",
    "# sort the features by importance\n",
    "feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print the feature importances\n",
    "for feature, importance in feature_importances:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")\n",
    "\n",
    "# select the top N features\n",
    "N = 10\n",
    "selected_features = [feature for feature, importance in feature_importances[:N]]\n",
    "\n",
    "# create a new DataFrame with only the selected features\n",
    "selected_features_df = df.select(*selected_features)\n",
    "selected_features_df.show()\n",
    "\n",
    "# keep the target column\n",
    "target_df = df.select(target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8008dfe-ace4-46b7-aa3d-76349cc2fe68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "5509e01c20228d3c6c77e41dcb54a0b87e287e3434aaded878c06aad1a3d666e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
